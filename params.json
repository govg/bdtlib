{
  "name": "Bdtlib",
  "tagline": "Library for Bandit learning routines",
  "body": "## Introduction\r\n\r\n[Multi-Armed Bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) originates from the study of slot machines and optimally playing them. The general scenario is where in an iterative process, an agent chooses from available actions, receives rewards corresponding to his choice, and attempts to maximize the reward he obtains. There are various assumptions that can be made on the actions (does the agent know any side information? can he see what reward he could have obtained with other choices?), which lead to different analysis. \r\n\r\nThis library implements popular algorithms that try to solve both the normal multi-armed bandits case (where the agent only choose an arm number, and each arm generates a reward distributed about a particular mean) and the contextual multi-armed bandit case (where the agent observes some side information, \"context\" associated with each arm before the choice is made). \r\n\r\n### Multi armed bandit without context\r\n\r\nThe currently implemented (and tested) algorithms are :\r\n* Upper Confidence Bound (UCB)\r\n* Epsilon Greedy\r\n* Random\r\n\r\n[This](https://www.cs.mcgill.ca/~vkules/bandits.pdf) gives a nice overview of the various algorithms, including ones that have not been implemented as of yet.\r\n\r\n### Contextual multi-armed bandits\r\nThe currently implemented (and tested) algorithms are:\r\n* [Thompson Sampling] (http://jmlr.csail.mit.edu/proceedings/papers/v28/agrawal13.pdf)\r\n* [LinUCB](http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf)\r\n* Epsilon Greedy - Currently in progress\r\n* Random \r\n\r\n[This](https://arxiv.org/pdf/1508.03326v2.pdf) is a nice overview of different algorithms and their analysis with respect to the contextual case.\r\n\r\nTwo popular datasets (that I will try and work with) that can be used as reference are the following\r\n* [Avazu CTR](https://www.kaggle.com/c/avazu-ctr-prediction/data)\r\n* [SoSo CTR](http://www.kddcup2012.org/c/kddcup2012-track2/data)\r\n\r\nNote that the (contextual) multi-armed bandit setting lends itself well to modelling online learning, especially for models where the final outcome (click/no click, expected rating) can be expressed as a linear combination of the input features (a'la linear regression). To this, it can be verified that these algorithms quickly converge to a decent linear regressor when used on standard regression datasets (the tests include one that uses the Boston House prices dataset).\r\n\r\n\r\n## Build and usage\r\n\r\nThe package has been tested only on Linux (Arch Linux 64bit), but it should work on any Linux distribution without any changes. I don't feel like testing on Windows. Usage is straightforward, drop the bdtlib folder into either your python path, or your local folder (in which case you will have to append that to the python path). Example code can be found in the tests folder, this includes testing all implemented algorithms as well as plotting the results on both the Boston prices dataset, as well as randomly generated linear model with uniform noise.\r\n\r\nAs this is part of a [course](http://www.cse.iitk.ac.in/users/purushot/courses/opt/2016-17-a/) project, I will be developing (making breaking changes to ) it in the coming month, and hopefully over the next few as well. \r\n\r\n## Contact\r\nI can be found using the nick govg on Freenode/Quakenet/Snoonet. \r\n\r\nFor further details, please go to my [homepage](http://home.iitk.ac.in/~govindg) \r\n\r\nGovind Gopakumar (@govg) \r\n\r\nDepartment of Computer Science and Engineering,\r\n\r\nIndian Institute of Technology, Kanpur\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}